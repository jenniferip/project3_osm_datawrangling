{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# London OpenStreeMap Data Analysis by Jennifer Penuelas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook documents my process of wrangling, cleaning, and exploring the OpenStreetMap data of London, England, United Kingdom. This is not an area I live in or have travelled to, which presented difficult when thinking about fixing problems outside of field type inconsistencies, spelling inconsistencies, and general user errors. Thus, no information was changed or added that could more accurately represent the area, as I am not familiar enough with it to suggest edits.\n",
    "    \n",
    "The data was obtained using Mapzen's metro extracts, specifically searching for London as the extract I was interested in. The search results are separated into two headers \"POPULAR EXTRACTS READY FOR DOWNLOAD NOW:\" and \"TO MAKE A CUSTOM EXTRACT:\", I worked with the London data linked first under the \"TO MAKE A CUSTOM EXTRACT:\" header. [Follow this link to search Mapzen metro extracts.](https://mapzen.com/data/metro-extracts/)\n",
    "\n",
    "The file size of the London extract I originally asked for is 1.25 GB but for the sake of this project and analysis, I worked on a sample of the London data which was of size 63.24 MB. \n",
    "\n",
    "Additionally, I would like to credit some of the ideas behind the outline of this analysis to [Carl Ward's sample project.](https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## London OSM Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The subsections below, give a general overview of the London data, as per the sample generated. \n",
    "\n",
    "In the subsections \"Unique Users:\", \"Tag Counts:\", and \"Key Types:\", I specify different outputs for a sample file that is about 120 MB as well as for the sample file of size 63.24 MB used for this analysis. This is because generating a sample file is quick whereas generating a database to query for things like \"Number of Nodes and Ways\" takes much longer. The files that give the information in the specified subsections work on the sample file and so they are quick to run. However, generating a database with the larger sample file is ridiculously slow so I had to trash it and use a smaller sample to build the database I queried. \n",
    "\n",
    "The time delay occurs because to build the database we first write the data into a schema that can be written into a tabular format which can then be easily inputted into the database. Then before writing the schema into tabular format, the schema is validated and this validation takes too long to do efficiently on a personal computer. However, since I did have the extra information from earlier runs of my code, I decided to include it rather than trash it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "### 1. File sizes:\n",
    "\n",
    "* london_data.osm - 1.25 GB \n",
    "* london_sample.osm - 63.24 MB\n",
    "> This sample was created by running `python sampling_osm.py` file with k=20, thus getting every 20th top level element. \n",
    "* nodes.csv - 21.52 MB\n",
    "* nodes_tags.csv - 19.11 MB\n",
    "* ways.csv - 2.59 MB\n",
    "* ways_tags.csv - 26.52 MB\n",
    "* ways_nodes.csv - 48.54 MB\n",
    "> All .csv files are created by running `python data.py`. Note, that this code takes quite some time to run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Unique Users:\n",
    "\n",
    "There are 3238 unique users, get this value by running `python users.py`.\n",
    "\n",
    "**EXTRA:** Running `users.py` on the larger sample file gives 4018 unique users. \n",
    "\n",
    "I would expect that as the sample size gets closer to the original size of the dataset, the difference in unique users would near zero whereas here the difference is 780 users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Number of Nodes and Ways:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 262,725 nodes with 60,636 node tags. \n",
    "\n",
    "There are 43,836 ways with 342,882 way nodes and 127,288 way tags. \n",
    "\n",
    "These were found using a query like this one: `SELECT count(*) FROM nodes;`. This query specifically gives you the number of nodes, to get all other values you would replace 'nodes' in the query with nodes_tags, ways_nodes, or ways_tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tag counts: \n",
    "\n",
    "The following dictionary is returned by the `count_tags` function in the `count_tags.py` file. Run the file by running `python count_tags.py` in the shell from within the folder containing the data files and the code files. This dictionary describes the types of tags we encounter in the data and how many of each there are. \n",
    "\n",
    "```\n",
    "{'member': 14718,\n",
    " 'nd': 342882,\n",
    " 'node': 262725,\n",
    " 'osm': 1,\n",
    " 'relation': 1062,\n",
    " 'tag': 193475,\n",
    " 'way': 43836}\n",
    "```\n",
    "\n",
    "**EXTRA:** For the larger sample file, the dictionary returned is: \n",
    "\n",
    "```\n",
    "{'member': 29566,\n",
    " 'nd': 685606,\n",
    " 'node': 525449,\n",
    " 'osm': 1,\n",
    " 'relation': 2124,\n",
    " 'tag': 386331,\n",
    " 'way': 87672}\n",
    "```\n",
    "\n",
    "In both cases, you see that the order of which tags you see the least to which you see the most is: osm, relation, member, way, tag, node, and nd. Between the larger and smaller file, the differences in the number of each tag you see are: osm - 0, relation - 1062, member - 14848, way - 43836, tag - 192856, node - 262724, and nd - 342724. In all cases, the number of times you see each tag basically doubles which makes sense since the size of the larger file is double that of the smaller file. However, I did not expect the pattern to be so uniform throughout the tag count.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Key types:\n",
    "\n",
    "Running `python key_types.py` prints out a dictionary of dictionaries which describe potential problems in the data with a count of how many times it encounters the specific probable problem and the k attribute value fitting each problem description for each `<tag>`. Below is a portion of what this dictionary looks like:\n",
    "\n",
    "```\n",
    "{'lower': {'count': 146878,\n",
    "           'text_values': set(['abutters',\n",
    "                               'access',\n",
    "                               'accessto',\n",
    "                               'active_traffic_management',\n",
    "                               'address',...])},\n",
    " 'lower_colon': {'count': 38287,\n",
    "                 'text_values': set(['abandoned:aeroway',\n",
    "                                     'abandoned:amenity',\n",
    "                                     'access:conditional',\n",
    "                                     'access:note',\n",
    "                                     'addr:apartment',...])},\n",
    " 'other': {'count': 8308,\n",
    "           'text_values': set(['BROADHAB',\n",
    "                               'CREATEDATE',\n",
    "                               'CREATEDBY',\n",
    "                               'FIXME',\n",
    "                               'FIXME:nsl',...])},\n",
    " 'problemchars': {'count': 2,\n",
    "                  'text_values': set(['hgv:7.5', 'maxweight:7.5'])}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extra:** The larger file has the following dictionary:\n",
    "\n",
    "```\n",
    "{'lower': {'count': 293473,\n",
    "           'text_values': set(['abutters',\n",
    "                               'access',\n",
    "                               'accessto',\n",
    "                               'active_traffic_management',\n",
    "                               'address',...])},\n",
    " 'lower_colon': {'count': 76097,\n",
    "                 'text_values': set(['abandoned:aeroway',\n",
    "                                     'abandoned:amenity',\n",
    "                                     'abandoned:highway',\n",
    "                                     'abandoned:sport',\n",
    "                                     'access:conditional',...])},\n",
    " 'other': {'count': 16759,\n",
    "           'text_values': set(['BROADHAB',\n",
    "                               'CREATEDATE',\n",
    "                               'CREATEDBY',\n",
    "                               'ENSISID',\n",
    "                               'FIXME',...])},\n",
    " 'problemchars': {'count': 2,\n",
    "                  'text_values': set(['hgv:7.5', 'maxweight:7.5'])}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things to notice:\n",
    "\n",
    "Although the larger file is twice the size, there are not twice the amount of possible 'problemchars'. That is a good thing! In my opinion it means that this data is well kept or contributed to and fixed often, especially since it is an area of popular interest. Additionally, notice that the values are potentially problematic because of the period in it so there is consistency within even the 'problemchars' thus allowing for more leniency when cleaning.\n",
    "\n",
    "Also, the text_values set in both of the 'lower_colon' and 'other' dictionaries from both files contain different values. Recall that when a sample is created, you decide the size by setting k equal to some value. The larger k is the smaller the sample, since k specifies which top level elements you want. In the smaller file, we set k = 20 thus getting every 20th element whereas the larger file was obtained by setting k = 10. Thus, the larger file contains the values from the smaller file as well where the smaller file contains every other value of that in the large file.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Additional Interesting Overview Facts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section there are a few interesting overview facts, based on my own personal interests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cafes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the queries: \n",
    "```\n",
    "    SELECT value, count(*) as search_amount \n",
    "    FROM nodes_tags\n",
    "    WHERE value = 'cafe';\n",
    "    \n",
    "    SELECT value, count(*) as search_amount \n",
    "    FROM ways_tags\n",
    "    WHERE value = 'cafe';\n",
    "    ```\n",
    "We find that there are 205 cafes, as the first query returns [(u'cafe', 128)] and the second returns [(u'cafe', 77)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abandoned Places! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below tells us that there are some abandoned places in our data, specifically the ones we see in our data are railways.\n",
    "\n",
    "Query: \n",
    "```\n",
    "    SELECT *\n",
    "    FROM ways_tags\n",
    "    WHERE value = 'abandoned';\n",
    "    ```\n",
    "Output:\n",
    "```\n",
    "    [(165810010, u'railway', u'abandoned', u'regular'),\n",
    "     (22617717, u'railway', u'abandoned', u'regular'),\n",
    "     (184920456, u'railway', u'abandoned', u'regular'),\n",
    "     (17548926, u'railway', u'abandoned', u'regular'),\n",
    "     (139309617, u'railway', u'abandoned', u'regular'),\n",
    "     (36854480, u'railway', u'abandoned', u'regular')]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Contributions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query `SELECT user, count(*) AS contributions FROM nodes GROUP BY user ORDER BY contributions DESC LIMIT 10;` tells us that the top ten users across the node elements are: \n",
    "```\n",
    "[(u'busdoc', 15378),\n",
    " (u'Rondon237', 8823),\n",
    " (u'SDavies', 8396),\n",
    " (u'Derick Rethans', 8286),\n",
    " (u'Tom Chance', 8136),\n",
    " (u'Paul The Archivist', 6998),\n",
    " (u'Amaroussi', 6474),\n",
    " (u'TimSC_Data_CC0_To_Andy_Allan', 6343),\n",
    " (u'Steve Chilton', 5820),\n",
    " (u'ecatmur', 5731)]\n",
    " ```\n",
    "Furthermore, the same query but from the ways table tells us that the top ten users contributing to way elements are:\n",
    "```\n",
    "[(u'busdoc', 2101),\n",
    " (u'SDavies', 1896),\n",
    " (u'Welshie', 1777),\n",
    " (u'Derick Rethans', 1755),\n",
    " (u'Amaroussi', 1601),\n",
    " (u'Paul The Archivist', 1425),\n",
    " (u'Tom Chance', 1209),\n",
    " (u'Rondon237', 1116),\n",
    " (u'harg', 959),\n",
    " (u'peregrination', 868)]\n",
    "```\n",
    "I find it interesting that the eight, ninth, and tenth contributors to node elements are not in the top ten for way elements. Also, that the third highest way contributor, 'Welshie', is not a a part of the top top ten for nodes. Now, recall from section three that there are significantly more nodes that ways, so the fact that some top node contributors are not in the top way contributors is expected. However, the the opposite is more surprising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Improvements to the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problems:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The 'user' and 'uid' values and columns created a problem both when generating the dictionaries to fit the schema to write the data into csv files as well as in querying.\n",
    "> a) When generating the csv files and inserting the data into the database, there were nodes which did not have 'user' and 'uid' attributes, creating a problem when trying to insert their values into the dictionaries.\n",
    "\n",
    "> b) When querying we saw user values like 'z\\u2006x\\u2006f\\u2006m' and '\\u042e\\u043a\\u0430\\u0442\\u0430\\u043d'. These types of user names are obviously not conciously created by a human user.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Node and way tags of type 'name' are problematic. They generally do not offer much understandable information, as such it is difficult to make changes that would make the information in these tags clearer. \n",
    "> a) The following query `SELECT key, value FROM nodes_tags WHERE type = 'name';` on both the nodes_tags table and the ways_tags tables give the following results:\n",
    "```\n",
    "[...\n",
    "  (u'ru', u'\\u0425\\u043e\\u043b\\u043b\\u043e\\u0443\\u044d\\u0439-\\u0440\\u043e\\u0443\\u0434'),\n",
    "  (u'eo', u'Londono'),\n",
    "  (u'cv', u'\\u041b\\u043e\\u043d\\u0434\\u043e\\u043d'),\n",
    "  (u'cdo', u'L\\xf9ng-d\\u016dng'),\n",
    "  (u'es', u'Londres'),\n",
    "  ...]```\n",
    " \n",
    "> and       \n",
    "```\n",
    "[...\n",
    "  (u'ru', u'\\u043c\\u043e\\u0441\\u0442 \\u0412\\u0430\\u0442\\u0435\\u0440\\u043b\\u043e\\u043e'),\n",
    "  (u'cy', u'Abaty San Steffan'),\n",
    "  (u'be', \n",
    "   u'\\u041f\\u0430\\u0440\\u043b\\u0430\\u043c\\u0435\\u043d\\u0442\\u0441\\u043a\\u0430\\u044f \\u043f\\u043b.'),\n",
    "  (u'en', u\"Bagley's Lane\"),\n",
    "  (u'fr', u'Abbaye de Westminster'),\n",
    "  ...]\n",
    "  ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Finally this data is organized to the point where there are some elements in which the k values signify a possible fix. The following query `SELECT key, value FROM nodes_tags WHERE type = 'fixme';` on both of the nodes_tags table and the ways_tags table with also with the condition `WHERE key = 'FIXME'` gives the areas that should be looked at for fixing and also a description of why or how they should be fixed.\n",
    "> a) The query `SELECT key, value FROM ways_tags WHERE type = 'fixme';` gives:     \n",
    "```\n",
    "[...\n",
    " (u'FIXME', u'Check this.'),\n",
    " (u'FIXME', u'yahoo shows a large island in the road?'),\n",
    " (u'FIXME', u'?identify?'),\n",
    " (u'FIXME', u'name? blurry photo'),\n",
    " (u'FIXME',\n",
    "  u'There seem to be too many restaurants here - some may have closed or been put in the wrong place.'),\n",
    "  ...]\n",
    "  ``` \n",
    "> and `SELECT key, value FROM ways_tags WHERE key = 'FIXME';` gives:\n",
    "```\n",
    "[...\n",
    "(u'FIXME', u'Trace from Yahoo Image.'),\n",
    " (u'FIXME', u'OS Locator says \"Hadden Road\"'),\n",
    " (u'FIXME', u'Trace from Yahoo Image.'),\n",
    " (u'FIXME', u'wheelchair tagging?'),\n",
    " (u'FIXME', u'double check this street name (bad photo)'),\n",
    "...]\n",
    "  ``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Fixes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1a) The simplest fix was to check if the node element had the attributes 'user' and 'uid'; if not, then the value of these would default to 'NO_USER' and 0, respectively. In the end, we find that there are only six nodes that required this alteration. The query `SELECT * FROM nodes WHERE user = 'NO_USER';` gives the following result: \n",
    "```\n",
    "[(17805033, 51.360339, -0.3631331, u'NO_USER', 0, 1, 128532, u'2006-10-07T20:08:04Z'),\n",
    " (17806151, 51.3622816, -0.3572716, u'NO_USER', 0, 1, 128532, u'2006-10-07T20:18:18Z'),\n",
    " (17806863, 51.3550082, -0.3375747, u'NO_USER', 0, 1, 128532, u'2006-10-07T20:30:28Z'),\n",
    " (17807424, 51.3654952, -0.355275, u'NO_USER', 0, 1, 128532, u'2006-10-07T20:44:26Z'),\n",
    " (17807459, 51.3650557, -0.3493271, u'NO_USER', 0, 1, 128532, u'2006-10-07T20:44:28Z'),\n",
    " (17916421, 51.4900279, -0.2815898, u'NO_USER', 0, 1, 129695, u'2006-10-08T23:18:04Z')]\n",
    " ```\n",
    " **NOTE:** The order of the columns is (id, lat, lon, user, uid, version, changeset, timestamp)\n",
    " \n",
    "Interestingly, these occurences have more than their made up user name and user id values in common. The version and changeset values are all the same and the timestamp values are all within minutes of each other. This leads one to question if these nodes are created when initiating the OpenStreetMap data for this particular region. I believe that this distintion, if it is true could be a valuable improvement for the data, then the 'user' value can go from the more ambiguous 'NO_USER' to 'SETUP'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Benefits:** Implementing this fix would make the data more clear upon first read through and would indicate some overall timeline within the data itself, particularly with the data initiation.\n",
    "\n",
    "> **Anticipated Problems:** This is more time consuming when creating the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1b) The query `SELECT user, count(*) AS contributions FROM nodes GROUP BY user ORDER BY contributions DESC;` allowed for the discovery of the suspicious user names referenced above. I did not fix them as these values were not disruptive in my own queries. However, if this would be a problem for anyone using this data, I suggest ruling out these potentially problematic username values by searching for them using regular expressions which look for '\\' or any other problematic characters in the node's user attribute value and then replacing those that are problematic to you with another easily recognizable value. Additionally, from a paranoid user, I would question the authenticity of the data from these users so maybe ignoring these rows completely could be a viable option and if these did occur from suspicious use of the OSM database can OSM catch it?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Benefits:** This fix makes the data clearer to read.\n",
    "\n",
    "> **Anticipated Problems:** This fix assumes a lot about problematic strings and the easiest fix, to replace them with something more noticeable, does create blindness to the problem in the analysis of your data. Additionally, creating some software that monitors suspicious activity or inputs to the OSM database is time consuming and maybe not worth it for this type of data contributions as their main information comes from users adding to it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2a) In my opinion, the data found through the queries above raise too many questions to be good data. Some of these questions are: What do the keys mean? Are the keys abbreviations or codes? If so, can I get access to them so that I can alter the data I work with and make it more informative at first glance. Then, again we see problematic strings for the value columns which contain '\\' characters so that the values are useless. What could these string be referring to? Finally, since these are tags of type 'name' what do the names refer to? To fix and maybe answer some of these questions, if the keys for name tags are codes, provide a way for decoding, even if it is to be done manually, with the original download extract and again, dealing with problematic string would be done in the way suggested in point 1b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Benefits:** This fix gives more documentation on the OSM data and how child tags are defined and it would make cleaning for people using the data much easier.\n",
    "\n",
    "> **Anticipated Problems:** This fix requires someone to document the data and how name type tags are defined, which is time consuming and perhaps not ideal especially since this is regional data which could change from area to area. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3a) Note that in the final query of this section the results mention Yahoo, which gives some indication that outside resources are somehow used to construct this data and could then be used to fix it without a human needing to conciously and manually fix the issues. Since there are latitude and longitudes associate with each element usually, the data could somehow use outside map applications and apps like Yelp to address some issues like for example whether a restaurant that used to be there is still open. It can see how much people still go to that location and make assumptions based on this information to deduce the answers they want. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Benefits:** This fix would hopefully solve some problems on its own and would use the power of other applications to create a more clear and accurate representation of the regional data.\n",
    "\n",
    "> **Anticipated Problems:** This fix would require a lot of permission from other apps to use their data and then would require a lot of work to build something smart enough to go through it and succesfully implement it into the OSM data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through, this analysis I generally found that the London data is well kept. There is consistency across the same attribute values and also consistency in things that may seem problematic. This was exemplified in both the 'Key Types' section where the dictionaries from both the smaller and larger file contain the same number and the same values for potentially problematic k values. Additionally, these 'key_type' dictionaries shed light how how well kept this London data is as there are tags with k values equal to 'FIXME' or 'fixme:date'. We can further see more values of these fix me tags by querying over the nodes_tags and ways_tags tables searching for type equal to fix me. I did audit a particular fix which pertained to date in the `audit.py` file but additionally I discussed this further in the 'Making Improvement's to the Data' section where we discussed the possibility of making other applications such as a maps app or Yelp, etc, fix our data for us. Finally, we also see the same consistency in the user columns of the nodes and ways tables and also the value columns of nodes_tags and ways_tags tables when the type is name where we see that potentially problematic user names, at the very least, contain the character '\\'.\n",
    "\n",
    "Some of my favorite discoveries are those made with the idea of being a potential tourist in mind. Particularly, my favorite discovery was made in noticing that the k attribute of elements could have values that included the term 'abandoned'. I then went on to search which locations were deemed abandoned. In my sample, I found only abandoned railways and I would be curious to see if a larger sample would reveal different abandoned places. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
